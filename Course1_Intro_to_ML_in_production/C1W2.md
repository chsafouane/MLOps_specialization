# Selecting and Training a Model

## Lecture 1: Modeling overview

## Lecture 2: Key challenges

AI system = Code (model/algorithm) + Data

**Challenges** in model development:

1. Doing well on training set
2. Doing well on dev/test sets
3. Doing well on business metrics/project goals: Doing well on dev/test sets doesn't mean doing well on business metrics

## Lecture 3: Why low average error isn't good enough

**Case 1: Performance on disproportionately important examples**

In the case of web search algorithm, the user wouldn't tolerate not getting the result she/he wants in case of navigational queries. The search engine will quickly lose the trust of its users in this case

![image-20210701173605599](../_assets/C1W2/image-20210701173605599.png)

**Case 2: Performance on key slices of the dataset**

Even if the ML model has high average testing accuracy, if it gives irrelevant predictions for some group then this might be unacceptable.

![image-20210701173940814](../_assets/C1W2/image-20210701173940814.png)

**Case 3: Rare classes**

Specifically in the case of **skewed data distribution** (imbalanced classes)

A related problem to this one is **accuracy in rare classes**. If someone has a rare medical condition, the model might do badly on it because it is not the present in the training dataset but from a medical standpoint, it might be unacceptable not to be detected. => *I think this one is the same as case 2.*

![image-20210701174410856](../_assets/C1W2/image-20210701174410856.png)

## Lecture 4: Establish a baseline

Establishing a baseline level of performance gives a point of comparison which helps decide where to focus one's efforts.

### Ways to establish a baseline

- Human level performance (HLP) => specially for unstructured data
- Literature for state-of-the-art/open source
- Quick-and-dirty implementation
- Performance of older system

In some case, baseline (e.g: HLP) also gives a sense of what is irreducible error/Bayes error.

## Lecture 5: Tips for getting started

- Literature search to see what's possible

- Find open-source implementations if available

- A reasonable algorithm with **good data** will often outperform a great algorithm with no so goo data

- Should deployment constraints should be taken into account? 

  - **Yes**, if baseline is already established and the goal is to build and deploy
  - **No (or not necessarily)**, if purpose is to establish a baseline and determine what is possible

- Sanity-check for code and algorithm

  

  ![image-20210701193102668](../_assets/C1W2/image-20210701193102668.png)





# Error analysis and performance auditing

## Lecture 1: Error analysis example

The process of error analysis is **iterative**. The error analysis is done on the **dev test**.

![image-20210701200008206](../_assets/C1W2/image-20210701200008206.png)

One may add tags and come up with new tags when doing error analysis. These tags are the categories that the model doesn't perform that good on and that we should improve the quality of the model on.

![image-20210701200726931](../_assets/C1W2/image-20210701200726931.png)

## Lecture 2: Prioritizing what to work on

### Room for improvement

In the table below, we consider the "Gap to HLP" as the **room for improvement** of our model as it represents the baseline in this example.

![image-20210701201110258](../_assets/C1W2/image-20210701201110258.png)

### Decide what category(ies) to improve

To prioritize what to work on, one has to decide which categories are most important to work on. The following are some criteria to take that decision

![image-20210701201513056](../_assets/C1W2/image-20210701201513056.png)

### How to improve specific categories

- Collect more data
- Use data augmentation to get more data
- Improve label accuracy/data quality

## Lecture 3: Skewed datasets

Use the confusion matrix, precision + recall and **F1-score** as well

If it's a multi-class problem, one can calculate these same metrics for all classes

Use the F1-score to prioritize which class to work on

## Lecture 4: Performance auditing

### Auditing framework

Brainstorm the ways the **system might go wrong** and establish **metrics** to assess performance against these issues on appropriate slices of data.

![image-20210701204543312](../_assets/C1W2/image-20210701204543312.png)



### Speech recognition example

![image-20210701204931590](../_assets/C1W2/image-20210701204931590.png)



# Data iteration

## Lecture 1: Data-centric AI development

In a model-centric view: Hold the data fixed and iteratively improve the code/model.

In a data-centric view: Hold the code fixed and iteratively improve the data

There is a **role for both views**.

## Lecture 2: A useful picture of data augmentation

When having more data, you pull up the specific point in the blue curve as well as nearby points to HLP curve. This gives an idea about which categories

![image-20210701215153811](../_assets/C1W2/image-20210701215153811.png)

